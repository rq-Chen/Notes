# Signal Processing

## Filtering and Spectral Leakage

- Why not use FFT to filter?

  - Filtering with FFT is to multiply the signal with a square window in the frequency domain, or convolute it with a `sinc()` funciton.

  - The frequency response of the sinc function is perfect for the given Fourier frequencies $$nf_0, n = 0, 1, \dots, f_s/2$$. However, its response "between" these discrete frequencies (e.g. $$1.5f_0$$) is not perfect. For example, you can compute a 512-point FFT on a 256-point data to see this different:

    ```matlab
    Y = [zeros(64,1); ones(129,1); zeros(63,1)];  % low-pass
    y = real(ifft(fftshift(Y)));
    for i = 1:3
        currPoint = 128 * 2 ^ i;
        newY = fft(y, currPoint);  % FFT with different precisions
        subplot(3, 1, i);
        plot(abs(newY(1:currPoint/2)));
        title(sprintf('%d points', currPoint));
        xlim([1 currPoint/2]);
        ylim([-0.5 2]);
    end
    sgtitle('FFT filtering');
    ```

    <img src = "FFT filtering.png" style = "zoom:50%" />

    So actually we cannot filter out the high-frequency components very well. See how this filter (cut-off at $$f_{65}$$) response to $$f_{65}$$ and $$f_{65.5}$$:

    ```matlab
    figure;
    x1 = sin(65 * 2 * pi / 256 * (0:255));
    X1 = ifftshift(fftshift(fft(x1)) .* Y');
    subplot(2, 1, 1); hold on;
    plot(0:255, x); plot(0:255, real(ifft(X1))); xlim([0 255]);
    title('f_{65}');
    
    x2 = sin(65.5 * 2 * pi / 256 * (0:255));
    X2 = ifftshift(fftshift(fft(x2)) .* Y');
    subplot(2, 1, 2); hold on;
    plot(0:255, x2); plot(0:255, real(ifft(X2))); xlim([0 255]);
    title('f_{65.5}');
    sgtitle('FFT low-pass filtering at f_{65}');
    ```

    <img src = "FFT filtering2.png" style = "zoom:50%" />

    You can see the fluctuation and *Gibbs phenomenon* at the ends (large oscillation near the jump point).

- Spectral leakage:

  - e.g., FFT a sine function with 1.5 cycle cannot give the power of 1.5cycle/trial, but only 1 cycle/trial or 2 cycle/trial
  - one way is to pad the data for enough long so the frequency resolution is high enough, although this will still generate nonrealistic high power on surrounding frequencies, e.g 1 and 2 



## Hilbert-Huang Transform

- Hilber-Huang Transform is to analyze the time-frequency feature of the *Intrinsic Mode Functions* of a signal by *Hilbert transform*, and these functions are generated by the *Empirical Mode Decomposition*
- Hilbert transform:
  - the *Hilbert transform* of a signal s(t) is its convolution (in the sense of Cauchy's principle value integral) with the Cauchy kernel $$\frac{1}{\pi t}$$
  - it can be understood as shifting the phases of the positive frequency components (by Fourier transform) by $$\pi/2$$ and those of the negative components by $$-\pi/2$$, which generates the *harmonic conjugate* of the original signal
    - $$\mathcal {F}{\bigl (}H(u){\bigr )}(\omega )=\mathop {\bigl [} \!-i\operatorname {sgn}(\omega ){\bigr ]}\cdot {\mathcal {F}}(u)(\omega )$$
    - some examples: $$\mathcal{H}[\sin(\omega t)] = -\cos(\omega t), \mathcal{H}[\cos(\omega t)] = \sin(\omega t), \omega > 0$$
  - note that this transform is linear, and the transform of a real function is also real. Taking advantages of this feature, we can analyze the *instantaneous phase and amplitude* of a signal by its *analytic representation* through Hibert transform (see below)
- Empirical Mode Decomposition:
  - Why we need EMD: directly extract the phases from the analytic representation of a signal may generate negative frequencies. Therefore, we can first decomposite the signal into several components whose analytic representation has monotonically increasing phases
  - Target of EMD - IMFs: Must satisfy two requirements
    - The difference between the number of IMF's zero point and peak point should be no more than 1 (namely, the function go upward and downward around x-axis like sin(x))
    - The envelope of IMF shoule be symmetric over x-axis
  - Pipeline of decomposition:
    - Compute the (positive and negative) envelope of the original signal y(t) and the mean of them $$y_1(t)$$, compute $$y_2(t) = y(t) - y_1(t)$$
    - if $$y_2(t)$$ statisfy the two requirements, then it is the first IMF; otherwise repeat the first step on $$y_2(t)$$
    - if we found an IMF, subtract it from the original signal and repeat the first two steps
    - Finally we get a certain number of IMFs and the residual



## Analytic Representation & Relevant Concepts

- Analytic representation:

  - the *analytic representation* $$s_a(t)$$ of a signal s(t) is to "cut" the negative frequency components (by Fourier transform) and "copy" their complex conjugate on the positive ones:

  $$
  S(f) = \mathcal{F}[s(t)], S_a(f) = \mathcal{F}[s_a(t)] \\
  S_a(f) = S(f) + \text{sgn}(f)S(f)
  $$

  - some examples: $$\cos(\omega t + \theta) \to e^{j(\omega t + \theta)}, \omega > 0$$

- Instantaneous frequency and phase:

  - the *analytic representation* can be plotted on the polar plain: $$s_a(t) = s(t) + j\hat{s}(t) = s_m(t)e^{j\phi(t)}$$. 
  - if s(t) is real, then the real part is just the original signal and the imaginary part is the *Hilbert transform* of the signal $$\hat{s}(t) = \mathcal{H}[s(t)]$$.
  - $$s_m(t)$$ is called *instantaneous amplitude* or *envelope* (即原函数的波包), $$\phi(t)$$ is called *instaneous phase* and its derivative is called *instaneous angular frequency*
  - **Note that the MATLAB function `hilbert(x)` computes the analytic representation of x rather than the Hilbert transform!!!**



## Wave Function

- Spherical wave:

  - A static spherical wave is modeled by $$e^{j\psi_s}$$, where $$\psi_s = \beta\cdot P_s$$. Here $$\beta$$ is the wave vector, pointing from the south pole to north pole (more exactly, $$\beta$$ is four-dimensional because there is a constant phase offset, so $$\psi_s = \beta_xP_x + \beta_yP_y + \beta_zP_z + \beta_0$$). For a travelling wave, $$\beta$$ changes with time.
  - It is somehow similar to throwing a stone at the north pole and the phase decreases as the wave travels along the latitude. So $$\beta$$ also represents the "source" of the wave and the moving direction of $$\beta$$ (after taking time into consideration) is exactly the moving direction of the TW.

- Fourier transform in both space and time: representing the activity of channel arrays by the summation of travelling waves with temporal frequency $$\omega$$ and wave vector (something like vector of wave numbers, representing the travelling direction of the wave) **k**
  $$
  \begin{align}
  \tilde{f}(\omega,\mathbf{k}) & \propto \int f(t, \mathbf{x}) \operatorname{e}^{j\omega t - j \mathbf{k}\cdot\mathbf{x}} \operatorname{d} t \operatorname{d}^3x \\
  f(t, \mathbf{x}) & \propto \int \tilde{f}(\omega,\mathbf{k}) \operatorname{e}^{-j\omega t + j\mathbf{k}\cdot \mathbf{x}} \operatorname{d}\omega \operatorname{d}^3k 
  \end{align}
  $$
  (More exactly, the wave number or spatial frequency $$\xi = |\bold{k}|$$)

- How to add a phase-lag to a narrow band signal: Just compute the analytic representation, add a lag to the instantaneous phase, and extract the real part.
  $$
  \begin{aligned}
  s_a(t) & = M(t)e^{j\theta(t)}, s_{1a}(t) = M(t)e^{j[\theta(t) - \phi_0]} \\
  s_1(t) & = \text{Re}[s_{1a}(t)] = M(t)\cos(\theta(t) - \phi_0) \\
  	& = M(t)\cos\theta(t)\cos\phi_0 + M(t)\sin\theta(t)\sin\phi_0 \\
  	& = s(t)\cos\phi_0 + \hat{s}(t) \sin\phi_0 \\
  S_1(f) & = \mathcal{F}[s_1(t)] = S(f)\cos\phi_0 + \hat{S}(f) \sin\phi_0 \\
  	& =
  	\begin{cases}
  		S(f)(\cos\phi_0 + j\sin\phi_0) & f < 0 \\
  		S(f)\cos\phi_0 & f = 0 \\
  		S(f)(\cos\phi_0 - j\sin\phi_0) & f > 0
  	\end{cases} \\
  	& =
  	\begin{cases}
  		S(f)e^{j\phi_0} & f < 0 \\
  		S(f)\cos\phi_0 & f = 0 \\
  		S(f)e^{-j\phi_0} & f > 0
  	\end{cases}	
  \end{aligned}
  $$
  We know that if we want to represent a signal as the summation of a series of cosine waves: $$s(t) = d_0 + d(f)\cos(2\pi ft + \phi(f)), f >0$$, then $$|d(f)| = 2|S(f)|, \phi(f) = \text{angle}(S(f))$$. Therefore, the phase of every cosine components in our $$s_1(t)$$ is shifted backward for $$\phi_0$$ comparing with s(t), which is consistent with our intuitive understanding about "phase lag". Besides, $$s_{1a}(t)$$ is indeed the analytic representation of $$s_1(t)$$.



## Power Spectrum & Coherence

- There are two definitions of *power spectral density*:

  - First, note that *power spectrum* here often (as in Matlab and Wiki) merely represents ***energy*** rather than ***energy/time***!
    - But for white noise, "power" really means energy/time, and it is the same as the variance of the noise amplitude
  - The square of Fourier coefficient:
    - For continuous signal:
      - $$X(\omega) = \int_{-\infin}^{\infin} x(t)e^{-j\omega t}dt$$, $$x(t) = \frac{1}{2\pi}\int_{-\infin}^{\infin}X(\omega)e^{j\omega t} d\omega$$
      - *Parseval's theorem* states that $$|X(\omega)|^2$$ is the energy density of a signal at frequency $$\omega$$: $$\int_{-\infin}^{\infin} |x(t)|^2 dt = \frac{1}{2\pi}\int_{-\infin}^{\infin} |X(\omega)|^2 d\omega = \int_{-\infin}^{\infin} |X(2\pi f)|^2 df$$
      - $$S_{xx}(\omega) = |X(\omega)|^2$$
    - For discrete signal:
      - $$X(\omega) = \sum_{n = 0}^{N - 1} x_ne^{-j\omega n\Delta t}$$, $$X(k) = \sum_{n = 0}^{N - 1} x_ne^{-j\frac{2\pi}{N}kn}$$, $$x_n = \frac{1}{N}\sum_{k = 0}^{N - 1} X(k)e^{j\frac{2\pi}{N}kn}$$
      - Parseval's theorem: $$\sum_{n = 0}^{N - 1}|x_n|^2\Delta t = \frac{1}{N} \sum_{k = 0}^{N - 1} |X(k)|^2 \Delta t$$
      - $$S_{xx}(\omega) = \frac{(\Delta t)^2}{T}|X(\omega)|^2 = \frac{1}{N}|X(\omega)|^2\Delta t$$ (if no sampling rate is provided, $$\Delta t$$ is ofen set to 1 and eliminated)
  - The Fourier transform of the *autocorrelation function* (see *cross-spectral density* below).

- *Cross-spectral density* or *cross power-spectral density* of x(t) and y(t):

  - Note: the defintion of cross-correlation function in Matlab is different that on Wiki (we use the Matlab version below).
  - Matlab: $$R_{xy}(\tau) = \int_t x(t+\tau)y^*(t) dt = \int_t x(t)y^*(t - \tau)dt$$, so a peak at $$\tau_0 $$ indicates $$x(t + \tau_0)$$ is similar to $$y(t)$$, i.e. x is behind y (x is y shifted rightward) by $$\tau_0$$
    - Wiki: See below, a peak at $$\tau_0$$ indicates x is ahead of y by $$\tau_0$$
    - $$R_{xy}$$ in Wiki is $$R_{yx}$$ in Matlab
  - It can be defined as the Fourier transform of their *cross-correlation function*:
  
  $$
  S_{xy}(\omega) = \int_{-\infin}^{\infin} \left[ \int_{-\infin}^{\infin}x(t)y^*(t - \tau)dt \right] e^{-j\omega \tau} d\tau
  $$
  
  - Or more directly, $$S_{xy}(\omega) = X(\omega)Y^*(\omega)$$, which can be deduced from the *cross correlation theorem*.
  
  - Therefore, the absolute value (amplitude) and angle of the cross-spectral density represents the combined (multiplied) power and phase lag of two signals at each frequency, respectively. For example:
    $$
    X(\omega) = Ae^{j\theta}, Y(\omega) = Be^{j\phi} \\
    S_{xx}(\omega) = A^2, S_{yy}(\omega) = B^2, S_{xy}(\omega) = ABe^{j(\theta - \phi)}
    $$
  
  - Therefore, the phase delay of Y relative to X is $$ang(S_{xy}(\omega)) = \omega\delta_t$$, where $$\delta_t$$ is the time delay. 
  
- *Coherence*, *complex coherence* and *magnitude-squared coherence* between x(t) and y(t):

  - The *Magnitude-squared coherence (MSC)* is defined as:
    $$
    C_{xy}(\omega) = \frac{|S_{xy}(\omega)|^2}{S_{xx}(\omega)S_{yy}(\omega)}
    $$

- But by this definition the MSC will always be equal to 1. So actually in application, all (auto or cross) power spectrums must be calculated by N data segments and then averaged (https://dsp.stackexchange.com/questions/10012/magnitude-squared-coherence-calculation-inconsistence):
  $$
  \begin{aligned}
  C_{xy}(\omega) & = \frac{|\sum_{n=1}^N X_n(\omega)Y_n^*(\omega)|^2}{\sum_{n=1}^N |X_n(\omega)|^2 \sum_{n = 1}^N |Y_n(\omega)|^2} \\
  	& = \text{squared cosine similarity between }\{X_n(\omega)\}_{n} \text{ and } \{Y_n(\omega)\}_{n}
  \end{aligned}
  $$
  Most frequently, this is done by the *Welch's method* for power spectrum estimation, which splits the data into overlapping segments, windows each segment, computes the *periodograms* (estimation of power density) and average all periodograms.

  - According to the *Cauchy-Schwarz inequality*, MSC ranges from 0 to 1. It estimates the extent to which y(t) may be predicted from x(t) by an optimum linear least squares function. For example, if y(t) is the output of a perfect LTI system with input x(t) without noise, then coherence will be 1.

  - In signal processing, *coherence* is often defined as $$\sqrt{C_{xy}(\omega)}$$, or sometimes $$C_{xy}(\omega)$$ itself. 

  - In neuroscience, *coherence* often means *complex coherence*:
  $$
    C'_{xy}(\omega) = \frac{S_{xy}(\omega)}{\sqrt{S_{xx}(\omega)S_{yy}(\omega)}}
  $$
  - By this definition, the phase delay of Y to X is the angle of the complex coherence. If we hypothesize that all the time delay is constant for all frequency, then it can be estimated by the regression slope of coherence versus frequencies.

- 1/f noise and oscillatory bands:

    - The 1/f noise is hypothesized to be related to the non-oscillatory population-level firing rate.
        - The slope of the 1/f noise between 30 and 50 Hz is a power-spectral correlate of synaptic excitation/inhibition balance.
        - A steeper slope corresponds to greater inhibition in a neuronal ensemble measured by the recording electrode.
    - The 1/f noise can be extracted by *irregular-resampling auto-spectral analysis* and subtracted from data. You may need to relu the result since there may be negative power.
    - You can extract the subject-dependent oscillation bands by estimating the (1/f removed) power spectrum with a *Gaussian mixture model* with several Gaussian bumps.
        - This may prevent the error by neglecting subjects' difference in theta/alpha/... band limits.

