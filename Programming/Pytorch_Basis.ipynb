{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('torch': conda)",
   "metadata": {
    "interpreter": {
     "hash": "c4fc55da78f512edaadb3d52b405c35dbb3a49a2b76b909d93261ff03cd19a0e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Building a Network with Pytorch\n",
    "\n",
    "A key feature in Pytorch is the *module*, which encapsulate computational units like layers and networks.\n",
    "\n",
    "A module is defined as a subclass of `nn.module`, which has a `.__init__(self)` method (of course) and a `.forward(self, x)` method:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# A layer\n",
    "\n",
    "class MyLinear(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_features, out_features))\n",
    "        self.bias = nn.Parameter(torch.randn(out_features))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return (input @ self.weight) + self.bias\n",
    "    \n",
    "# A network\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l0 = MyLinear(4, 3)   # use self-defined layers\n",
    "        self.l1 = nn.Linear(3, 1)  # use predefined layers\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.l0(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.l1(x)\n",
    "        return x"
   ]
  },
  {
   "source": [
    "- In `.__init__`, you initialize the module and define the trainable modules/parameters\n",
    "  \n",
    "  - The immediate submodules can be shown by generators `.children()` or `.named_children()`, and all submodules can be shown recursively by `.modules()` or `.named_modules()`\n",
    "\n",
    "  - All parameters can be shown by `.parameters()` or `.named_parameters()`\n",
    "  \n",
    "- In `.forward()`, you define the computational flow of the module"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nChildren:\n\nMyLinear()\nLinear(in_features=3, out_features=1, bias=True)\n\nNamed_modules:\n\n('', Net(\n  (l0): MyLinear()\n  (l1): Linear(in_features=3, out_features=1, bias=True)\n))\n('l0', MyLinear())\n('l1', Linear(in_features=3, out_features=1, bias=True))\n\nParameters:\n\nParameter containing:\ntensor([[ 0.1162, -2.2683,  0.5358],\n        [-0.2739, -0.7976, -0.3329],\n        [ 0.1297, -1.6810, -0.6916],\n        [ 0.6894,  0.7266,  0.3310]], requires_grad=True)\nParameter containing:\ntensor([ 0.2122,  0.3660, -0.0027], requires_grad=True)\nParameter containing:\ntensor([[ 0.4613,  0.2316, -0.2788]], requires_grad=True)\nParameter containing:\ntensor([0.0131], requires_grad=True)\n\nNamed_parameters:\n\n('l0.weight', Parameter containing:\ntensor([[ 0.1162, -2.2683,  0.5358],\n        [-0.2739, -0.7976, -0.3329],\n        [ 0.1297, -1.6810, -0.6916],\n        [ 0.6894,  0.7266,  0.3310]], requires_grad=True))\n('l0.bias', Parameter containing:\ntensor([ 0.2122,  0.3660, -0.0027], requires_grad=True))\n('l1.weight', Parameter containing:\ntensor([[ 0.4613,  0.2316, -0.2788]], requires_grad=True))\n('l1.bias', Parameter containing:\ntensor([0.0131], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "myNet = Net()\n",
    "\n",
    "print('\\nChildren:\\n')\n",
    "for Child in myNet.children():\n",
    "    print(Child)\n",
    "\n",
    "print('\\nNamed_modules:\\n')\n",
    "for Module in myNet.named_modules():\n",
    "    print(Module)\n",
    "\n",
    "print('\\nParameters:\\n')\n",
    "for Para in myNet.parameters():\n",
    "    print(Para)\n",
    "\n",
    "print('\\nNamed_parameters:\\n')\n",
    "for Para in myNet.named_parameters():\n",
    "    print(Para)"
   ]
  },
  {
   "source": [
    "You can use modules dynamically with the help of `.ModuleList()` or `.ModuleDict()`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nNamed_modules:\n\n('', DynamicNet(\n  (linears): ModuleList(\n    (0): MyLinear()\n    (1): MyLinear()\n    (2): MyLinear()\n  )\n  (activations): ModuleDict(\n    (relu): ReLU()\n    (lrelu): LeakyReLU(negative_slope=0.01)\n  )\n  (final): MyLinear()\n))\n('linears', ModuleList(\n  (0): MyLinear()\n  (1): MyLinear()\n  (2): MyLinear()\n))\n('linears.0', MyLinear())\n('linears.1', MyLinear())\n('linears.2', MyLinear())\n('activations', ModuleDict(\n  (relu): ReLU()\n  (lrelu): LeakyReLU(negative_slope=0.01)\n))\n('activations.relu', ReLU())\n('activations.lrelu', LeakyReLU(negative_slope=0.01))\n('final', MyLinear())\n\nNamed_parameters:\n\n('linears.0.weight', Parameter containing:\ntensor([[ 0.1599,  0.4401, -0.8715,  1.4173],\n        [ 0.3116, -0.0307,  0.2817,  0.8797],\n        [-0.2651, -0.1729, -0.5786,  0.3769],\n        [-0.8076,  1.2252, -1.8202,  0.3763]], requires_grad=True))\n('linears.0.bias', Parameter containing:\ntensor([-1.0593,  0.0390, -1.2481,  2.0366], requires_grad=True))\n('linears.1.weight', Parameter containing:\ntensor([[-0.9625, -0.4335,  1.2135,  1.1254],\n        [ 0.8666,  1.6099,  0.9853, -1.0916],\n        [-1.0528, -1.4472,  0.3420, -0.5608],\n        [-1.8714,  1.4668, -0.9039,  1.1514]], requires_grad=True))\n('linears.1.bias', Parameter containing:\ntensor([-0.1988, -0.4683, -1.0365,  1.2375], requires_grad=True))\n('linears.2.weight', Parameter containing:\ntensor([[-0.1972, -0.6334, -0.3720,  0.1932],\n        [ 0.7833,  0.1286,  0.7229, -0.0248],\n        [-0.3022,  0.8636,  0.1963,  0.8229],\n        [ 0.7644,  0.5498, -1.0856, -1.5646]], requires_grad=True))\n('linears.2.bias', Parameter containing:\ntensor([-0.9603,  0.4969, -0.2008, -1.1246], requires_grad=True))\n('final.weight', Parameter containing:\ntensor([[-0.0243],\n        [-1.4881],\n        [ 1.7383],\n        [ 1.1939]], requires_grad=True))\n('final.bias', Parameter containing:\ntensor([-0.8339], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "class DynamicNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_layers):\n",
    "        super().__init__()\n",
    "        self.linears = nn.ModuleList(\n",
    "            [MyLinear(4, 4) for _ in range(num_layers)])\n",
    "        self.activations = nn.ModuleDict({\n",
    "            'relu': nn.ReLU(),\n",
    "            'lrelu': nn.LeakyReLU()\n",
    "        })\n",
    "        self.final = MyLinear(4, 1)\n",
    "\n",
    "    def forward(self, x, act):\n",
    "        for linear in self.linears:\n",
    "            x = linear(x)\n",
    "        x = self.activations[act](x)\n",
    "        x = self.final(x)\n",
    "        return x\n",
    "\n",
    "dynamic_net = DynamicNet(3)\n",
    "sample_input = torch.randn(4)\n",
    "output = dynamic_net(sample_input, 'relu')\n",
    "\n",
    "print('\\nNamed_modules:\\n')\n",
    "for Module in dynamic_net.named_modules():\n",
    "    print(Module)\n",
    "\n",
    "print('\\nNamed_parameters:\\n')\n",
    "for Para in dynamic_net.named_parameters():\n",
    "    print(Para)"
   ]
  },
  {
   "source": [
    "To train a network:\n",
    "\n",
    "- Register the parameters into an optimizer\n",
    "\n",
    "- Repeat the following:\n",
    "\n",
    "  - Feed one sample into the network\n",
    "\n",
    "  - Calculate the loss\n",
    "\n",
    "  - Clear current gradient, then BP\n",
    "  \n",
    "  - Update the optimizer state (i.e. the parameters)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the network (from previous section) and optimizer\n",
    "net = Net()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-4, weight_decay=1e-2, momentum=0.9)\n",
    "\n",
    "# Run a sample training loop that \"teaches\" the network\n",
    "# to output the constant zero function\n",
    "for _ in range(10000):\n",
    "    input = torch.randn(4)\n",
    "    output = net(input)\n",
    "    loss = torch.abs(output)\n",
    "    net.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ]
}